# Compiler-cuDNN integration from first principles

Back in the days of yore, deep learning compilers could implement convolutions on NVIDIA GPUs in one of two ways: code generation and calls to libraries like cuDNN and cutlass.  Compilers had to choose; code generation enabled bespoke fusion but did not leverage the engineering work that has gone into cuDNN, while cuDNN/cutlass limited fusion opportunities but used the highly optimized kernels in these libraries.

cuDNN 8.0 changed all that by introducing a "Graph API".

The cuDNN Graph API provides a declarative programming model for describing and executing a graph of operations.  Unlike the fixed set of fusion patterns the legacy cuDNN APIs provide, these graphs can represent bespoke fusions.

In our GTC talk, we introduce a clean integration of the cuDNN graph API as an MLIR dialect and a lowering from the TCP (Tensor Compute Primitives) dialect to this cuDNN dialect.  We show an end-to-end example of our conception of a first-class compiler integration with this API.

Through this integration, we blur the line between compilers and libraries and enable compilers to benefit from the optimized kernels in cuDNN without giving up on fusion.

# Pointers to code

* cuDNN dialect is being incubated in the OpenXLA project and we are working with the community to graduate it over there.

* TCP is being developed in the [mlir-tcp](https://github.com/llvm/torch-mlir/tree/mlir-tcp) branch of TorchMLIR. Some of the updates to lower to cuDNN are yet to land in this branch.


# Current status

We have an MLIR pipeline that performs the following lowerings:
```
Torch -> TCP -> cuDNN / Linalg -> LLVM
```
The runtime part needed to execute cuDNN is being actively worked on.

Here we show an example of the kind of cuDNN integration that we can do with this work. Lets start with the following ops from resnet18. 

```
%109 = torch.aten.relu %108
        : !torch.vtensor<[1,64,56,56],f32> -> !torch.vtensor<[1,64,56,56],f32>
%110 = torch.aten.convolution %109, %88, %none, %101, %101, %101, %false, %102, %int1
        : !torch.vtensor<[1,64,56,56],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none,
          !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>,
          !torch.int -> !torch.vtensor<[1,64,56,56],f32>
```

They get lowered to TCP first.

```
%139 = tcp.clamp %138 {min_float = 0.000000e+00 : f32}
        : tensor<1x64x56x56xf32> -> tensor<1x64x56x56xf32>
%140 = tcp.conv2d %139, %110 
        {dilation = array<i64: 1, 1>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}
        : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

These ops are then fused into a group in TCP, specifically to target cuDNN.

```
%40 = tcp.isolated_group %39, %15 {
^bb0(%arg1: tensor<1x64x56x56xf32>, %arg2: tensor<64x64x3x3xf32>):
    %171 = tcp.clamp %arg1 {min_float = 0.000000e+00 : f32}
            : tensor<1x64x56x56xf32> -> tensor<1x64x56x56xf32>
    %172 = tcp.conv2d %171, %arg2
            {dilation = array<i64: 1, 1>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}
            : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
    tcp.yield %172 : tensor<1x64x56x56xf32>
} : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

The group is then lowered to the cuDNN dialect.

```
%40 = cudnn.get_current_handle !cudnn.handle
%41 = cudnn.build_and_exec_graph %40 %39, %15 {
^bb0(%arg1: !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>,
     %arg2: !cudnn.tensor_desc<64x64x3x3xf32, alignment = 1, stride = []>):
    %179 = cudnn.pointwise_relu(%arg1) type = f32 lower_clip = 0.000000e+00
            : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
            -> !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
    %180 = cudnn.convolution(%179, %arg2)
            type = f32 alpha = 1.000000e+00 beta = 0.000000e+00 spatial_dim_count = 4
            spatial_stride = [1, 1] pre_padding = [1, 1] post_padding = [1, 1]
            dilation = [1, 1]
            : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>,
              !cudnn.tensor_desc<64x64x3x3xf32, alignment = 1, stride = []>
            -> !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
    cudnn.build_graph(%180) : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
 } : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

# Communication channels

* Discord for [cuDNN](https://discord.gg/HeGBk8Jk) dialect
* Discord for [TCP](https://discord.gg/DjwgYMnj) dialect
